train tip transform model martin charl univ faculti mathemat physic institut formal appli linguist pragu abstract articl describ experi neural machin translat use recent ten examin critic paramet translat qualiti memori usag train stabil train time conclud experi set fellow research addit con gener mantra data larger address scale multipl provid practic tip prove train regard batch size learn rate warmup step maximum sentenc length given particular hardwar data constraint introduct alreadi clearli establish neural machin translat new state art machin translat see recent evalu campaign underli neural network architectur nevertheless still frequent di cult predict architectur best combin properti model size stabil speed train also practic avail consist train data sensit noi data also wide rang hyper paramet train right set turn often critic compon success may toolkit explor want empir explor import hyper paramet hope observ use also research consid model framework le et al either mostli theoret experi support domain like imag recognit rather machin translat articl gap focu exclu mt transform model provid hope best practic particular set observ con rm gener wisdom larger train data observ somewhat surpri two three time faster singl gpu interact maximum sentenc length learn rate batch size prepar section main contribut articl set comment experi set recommend final sec conclud section evalu methodolog machin translat evalu mani way form human judgment alway use ultim resolut applic one human refer translat widespread automat problem better perform altern et al simplic stick evalu result also use ident signatur score report paper automat download refer translat given consid stop criterion situat complic fact train esp hardli practic machin learn evalu model test set start bit sooner thu applic practic mani paper neural machin translat specifi stop criteria whatsoev sometim train et al sometim exact number train step given indic much model point evalu point approxim stop criterion rather riski comparison fair somewhat reliabl method keep train ed number iter certain number epoch howev perfect solut either model quit converg time di perform larg quit possibl complex model would need epoch eventu arriv higher score competitor also durat one train step one epoch di model wall clock time tri standard techniqu earli stop delta hyper paramet di random seed moreov get best result would use larg small delta paramet may result chang random initi mean translat qualiti test set begin worsen train loss keep improv final choic full learn curv singl score solut fulli prevent risk prematur judgment sudden twist result case plot case insensit score wall clock time hour solut obviou depend hardwar chosen alway driver variat measur unfortun unavoid could fulli isol comput di process machin gener network c base experi replic experi variat neglig terminolog clariti de ne follow term adher rest paper translat qualiti automat estim well translat carri particular model express mean sourc estim translat qualiti sole score one refer translat train step denot number iter number time updat run number also equal number mini batch process batch size sequenc sequenc model batch size usual ed number howev es approxim number token one batch allow use higher number short sentenc one batch smaller number long sentenc e batch size number train exampl consum one train step languag one step train epoch correspond one complet pass train data easi measur number train epoch purpo number token sentenc de maximum sourc target pad symbol howev pad still need thu approxim actual number non pad batch number train step order convert train step epoch need multipli step e batch size divid must thu comput special script comput speed com speed obviou depend hardwar gpu speed commun softwar driver version librari version mentat main paramet comput speed model size network train throughput amount train data digest train report train throughput per hour train throughput equal comput speed multipli e batch size converg speed converg speed chang heavili train start high decrea train progress converg model speed zero time till score train time need achiev certain level translat qualiti case use inform measur clear de ne moment given score de ne time never fall given level exampl till score number train exampl need achiev certain level equal time till score multipli train throughput tool evalu within implement provid nice visual train progress origin implement optim toward speed evalu rather toward follow standard eld thu never expo user actual instead word accord result usual time higher real due depend train data vocabulari easili reproduc vari experi thu suitabl report public de time till score lead high varianc valu rel high varianc subsequ checkpoint visibl learn curv decrea variat one use bigger develop test set en word cs word k k k total tabl train data resourc give result script use two way evalu one translat evalu translat given directori creat store result event articl creat way script checkpoint given directori averag window n subsequ check point respect detail averag see section data select focu english czech translat direct train data come parallel version sentenc pair rest sentenc pair come three smaller sourc news commentari common crawl detail tabl use sentenc pair experi experi section substitut older consid smaller et al contain sentenc pair word plot perform throughout train use develop set overlap train data section appli best model judg perform develop set comparison state art system three script master three script use still progress wait given number minut new checkpoint appear subset et al train data data alway scale realist data size achiev introduct unit leav train neural network feed origin input possibl see lee et al adopt vision support use extern unit come built method similar word piec algorithm wu et al later evalu data follow default provid raw plain text train sentenc use default paramet share sourc target english czech vocabulari size total number main train data million take maximum english czech length sentenc pair need comput number epoch see section smaller million case averag number per space delimit word consid thu provid set technic tip tip train data train data batch size higher exclud train sentenc longer given threshold control paramet see smaller process bit faster vs attach deliv project control constant must chang directli sourc code log low vocabulari estim word seen twice experi experi section present sever experi alway summar give gener applic tip learn experi done unless state otherwi experi two set hyper paramet tran big base di er mainli size model note tran name set hyper paramet appli even train multipl experi see section use model default see discuss di size section need forc checkpoint save hour see section disabl intern evalu thu make train bit faster see section comput speed train throughput primarili interest translat qualiti learn curv three import factor batch size number use model size speed usual almost constant given experi tabl show comput speed train throughput singl gpu variou batch size model size base big base model allow use higher batch size big model cell big model result out memori error mark see experi report better tran even train although name suggest author opposit experi see show statist comput speed curv curv checkpoint save comput speed thu much slower experi use order abl test bigger batch size howev addit experi check train throughput base big comput speed model base big b train throughput tabl comput speed train throughput singl gpu comput speed decrea increa batch size oper gpu fulli train throughput grow sub linearli increa batch size base experi small advantag set batch size maximum valu return question section take account translat qualiti also see base model approxim two time bigger throughput well comput speed rel big model tabl comput speed train throughput variou number big model tabl use big model vari number overhead gpu synchron appar decrea com speed nevertheless train throughput still grow process time train data per hour rel singl gpu without overhead would hypothet expect time data train time hour train time day train sentenc train sentenc figur train data size e learn curv main train million sentenc pair altern train million sentenc pair train big model overhead scale multipl smaller overhead scale higher batch size scale singl gpu increa throughput time scale batch size singl gpu increa throughput time train data size word er train data baselin versu smaller train hardwar hyper paramet big train smaller time smaller number word converg two day train improv next week train train bigger give slightli wor result eight hour train shown graph clearli better result two day train reach eight day bigger mean epoch smaller need reach converg also moment bigger compar two also anoth experi two gave slightli wor result two day train clearli better result eight day hypoth somewhat cleaner howev enough reach converg reach converg tip train data size compar di smaller cleaner vs bigger noisier need train long enough result hour day train singl gpu may mislead larg train data half improv even one week train eight day train two anoth experi easili interpol one result anoth smaller train data converg day main train data time bigger continu improv even day model size choo right model size import practic reason larger model experi two model big base di er four hyper paramet summar tabl model base big big base hyper paramet di con experi howev base model take restrict number epoch need reach point small train set tri also model three time larg base time larg big reach better result big report train time hour big model batch size gpu base model batch size gpu base model batch size gpu figur e model size batch size singl gpu train time hour big model batch size base model batch size figur e model size batch size less one day train big batch size becom better base batch size even anoth experi di grow three day train better base batch size hour train tip model size big base model plan train longer one day gb memori avail gpu less memori benchmark big base maximum possibl batch size batch size longer sentenc train test none maximum sentenc length model size base big optim adam last two column show percentag sentenc train test data longer given threshold tran maximum train sentenc length complet ed default use instead sinc transform implement suddenli run memori even sever hour train good know larg batch size ts gpu tabl present empir measur base big model adam variou valu set bia translat toward shorter sentenc would hurt tran qualiti last two column tabl show set resp result exclud resp sentenc train data resp sentenc develop test data longer detriment e smaller train data length bia minim set howev experi figur show strang drop one hour train experi lower even learn curv wor time smaller model adam store second moment weight leav experi futur work train time hour max length max length max length max length max length max length figur e restrict train data variou valu train singl gpu big model experi without shown curv restrict stop improv hour train show sudden increa case diverg train discuss section learn rate high explan phenomenon anoth set experi vari time instead case still result restrict case batch size high enough almost e check new train sever model variou three day observ train even chang decod paramet alpha tip reason low allow use higher batch size prevent out memori error sever hour train also higher percentag train sentenc almost long high never otherwi train time hour base batch size base batch size base batch size base batch size base batch size figur e batch size base model train singl gpu batch size except default howev recommend alway set batch size explicitli least make note default given version report experi result experi singl gpu base model higher batch clearli better term measur time till score need day exampl time longer time exampl re plot figur step instead time x axi di curv would even bigger tabl know bigger batch slightli higher train throughput re plot number exampl process x axi di smaller still visibl batch size ed see advantag use power two valu experi figur use got curv re run without restrict except fail train time hour big batch size big batch size big batch size big batch size big batch size big batch size figur e batch size big model train singl gpu batch size base model higher batch size give better result although return observ goe common knowledg framework deep learn gener et al smaller batch higher test set end experi base model bigger batch faster train throughput could expect also faster converg speed time till score exampl till score interestingli replic experi big model see quit di case sharp di batch size train well drop two hour train recov slowli accord smith le smith et al gradient noi scale dynam learn rate divid batch size cf section thu lower diverg perman case batch size figur temporari case batch size continu grow temporari drop much slowli non diverg curv sure cau di base big model regard sensit batch size one hypothesi big model train time hour learn rate learn rate learn rate learn rate learn rate figur e learn rate singl gpu train default batch size warmup step di cult initi thu sensit diverg earli train phase also base increa batch size highli help big limit may minim batch size need prevent diverg train tip batch size size set high possibl keep reserv hit out memori error advi establish largest possibl batch size start main long train learn rate warmup step singl gpu default learn rate translat model figur show vari valu within rang make almost di set rate high shown result mean almost zero stay forev common solut prevent diverg train decrea paramet increa introduc gradient clip paramet con schedul set default big model mean within schedul call version older train time hour warmup step warmup step warmup step warmup step warmup step figur e warmup step singl gpu train default batch size learn rate root decay t cf section step actual learn rate thu highest diverg happen usual happen within hour train actual learn rate becom highest increa warmup step abl train learn rate even without diverg learn curv look similarli curv baselin set high green curv result slightli slower converg match baselin hour train conclud singl gpu big model rel default valu within rang tip learn rate warmup step case diverg train tri gradient clip warmup step total train step tri decrea learn rate decrea warmup step keep learn rate also increa maximum actual learn rate way lin aka schedul implement number allow train multipl machin simpli use paramet explain section paramet interpret per gpu e batch size time bigger two batch size four batch size e batch size three case con empir mean train loss versu train step x axi section figur show curv di number big model batch size learn rate warmup step default valu respect faster achiev never fall hour tabl time train data consum reach time till score train without clearli surpass threshold alreadi outsid figur hold least version somewhat user may futur see make sure environ variabl set enough card visibl allow also distribut train multipl machin experi singl machin distribut train use synchron adam updat default train time hour train time day figur e number mark black line see two three time faster singl gpu measur time till score need much less train exampl similarli time less train data need recal figur shown increa batch size howev batch size use improv higher could expect higher throughput quit surpri especi consid fact tune learn rate warmup step see next section tip number fastest converg use mani avail hold even experi done exampl better run one experi anoth rather run two experi parallel eight singl gpu experi parallel would interest tri simul multi gpu train singl gpu simpli use ghost batch size higher actual batch size leav futur work learn rate warmup step multipl relat work focu mostli question adapt learn rate schedul scale one gpu devic gener say theori suggest multipli batch size k one multipli learn rate keep varianc gradient expect without actual explain theori suggest howev multipli learn rate k without explan detail di linear scale heurist becom popular lead good scale result practic interpret sgd gradient noi scale n b learn rate train set size bi e batch size noi drive sgd away sharp therefor optim batch size maxim test set accuraci word keep optim level gradient noi lead gener well need scale learn rate linearli increa e batch size howev ho provid theoret empir support claim show w w nb updat step win rang e batch size b need scale b better linear scale et al con rm linear scale perform well suggest use layer wise adapt rate scale suggest appli also sequenc sequenc task self attent network transform sever di well io seem import scale transform use ho introduc so call learn rate given number step instead epoch normal lei ba et al transform use adam togeth inver squar root learn rate decay paper use sgd momentum piecewi constant learn rate decay experi decid empir optim learn rate train increa learn rate result diverg train drop almost two hour train similarli singl gpu experi section abl prevent diverg increa abl use learn rate increa led diverg anyway howev none experi led improv default learn rate curv hour train et al show invari simultan rescal learn rate batch size break learn rate get larg batch size get small similar observ report et al thu initi hypothesi maxim learn rate suitabl stabl train experi even scale singl report tabl answer riddl need understand learn rate schedul implement parametr learn rate schedul work learn rate schedul paramet actual inter number epoch train exampl exampl popular setup learn rate factor epoch howev increa e batch size time use instead step batch normal di cult transform use still success switch batch normal possibl ghost batch normal due loss error cay piecewi constant decay see implement paramet stay case step constant contain also paramet divid paramet achiev actual learn rate given number train exampl origin singl gpu set explain riddl previou section keep paramet scale bigger e batch actual increa hold aka schedul ignor warmup step want keep learn rate also warmup phase would need divid warmup step k howev mean maximum actual learn rate higher rel singl gpu maxim actual learn rate lead diverg experi deed mani research et al suggest use warmup scale order prevent diverg transform use learn rate warmup default even singl gpu train cf section make sen use warmup train exampl multi gpu set experi default learn rate use warmup step instead default e curv bit higher hour afterward decrea warmup step result retard curv complet diverg tip learn rate warmup step multipl paramet optim valu found singl gpu experi expect improv way resum train checkpoint need continu train di machin hyper paramet search want continu promi setup save also adam addit suggest scale ho er et al show fulli close need train longer absolut number step updat matter point view use step instead epoch time paramet learn rate schedul may complet wrong idea train time hour averag checkpoint averag checkpoint averag figur e checkpoint averag train stop howev stop resum train also exploit chang hyper paramet meta parametr number step exampl smith train instead decay learn rate yet anoth usag domain adapt switch larg gener domain train data small target domain train data last epoch store checkpoint make sure learn rate small checkpoint averag interv use accord experi slightli also advantag less time spent checkpoint save train faster figur show e averag twofold averag curv lower varianc checkpoint checkpoint almost alway better baselin without averag usual setup seen improv due averag earli phase train baselin learn curv grow fast better use fewer automat score ave ave z ter beer system day onlin b cu chimera onlin manual score rank automat metric provid ter metric lower better best result bold second best ital checkpoint averag later phase shown figur day train seem checkpoint cover last hour give slightli better result averag checkpoint done proper evalu use pair bootstrap test hour summar result fact resum train start random posit train data cf section actual exploit train get two copi model train number step checkpoint semi independ model averag way help bit top checkpoint averag tip checkpoint averag averag checkpoint take minut boost compar time need whole train tool automat checkpoint averag evalu describ section comparison system tabl provid result english czech news translat task best transform model big train day averag checkpoint evalu use exact implement automat metric automat evalu fulli reliabl see high score cu chimera despit lower manual rank see transform model outperform best system ter beer despit use back translat data model right left transform use subset constrain train data result compar conclu present broad rang basic experi transform model et al english czech neural machin translat limit explor less basic paramet set believ report use research sum experi done articl took year gpu time among practic observ seen transform model larger batch size lead faster train importantli better tran qualiti given least day gpu train larger setup big also best use mani possibl singl gpu experi concurr form winner number automat metric acknowledg research support grant czech scienc resourc distribut project ministri educ youth sport czech republ bibliographi neural machin translat jointli learn align translat proceed petra martin david martin joy par proceed eighth intern languag resourc istanbul turkey euro pean languag resourc associ isbn tom martin roman enlarg czech english parallel corpu process tool ivan editor text speech dialogu intern confer number lectur note intellig page masaryk univ springer intern publish isbn matthia huck raphael copenhagen denmark septemb yvett graham amir result metric share task proceed second confer machin translat copenhagen denmark septemb stochast gradient descent trick page springer berlin heidelberg berlin heidelberg isbn url l f e curti j optim method larg scale machin learn e print june url mauro federico luisa jan sebastian christian overview page tokyo japan accur train hour url ho er daniel train longer gener better close gener gap larg batch train neural network i u v s h r fergu s r garnett editor ad neural inform process system page associ inc url io e url zachari kenton nicola fischer amo j three factor sgd url jorg mikhail ping peter tang larg batch train deep learn gener gap sharp proceed url alex one weird trick parallel neural network url jason thoma fulli charact level neural machin translat without explicit segment url lei ba j j r g e hinton layer normal e print juli todd ward method au evalu machin translat proceed page philadelphia pennsylvania lisbon portug septemb url rico barri alexandra birch neural machin translat rare word unit proceed page berlin germani august url n m stern adapt learn rate memori cost e print apr url smith samuel l v le bayesian perspect gener stochast gradient descent proceed second workshop bayesian deep learn nip long beach ca usa url smith samuel l v le decay learn rate increa batch size url jone n gomez i s h r fergu s r garnett editor advanc neural inform process system page associ inc url wu mike chen v le mohammad wolfgang young jason smith hugh dean url yang igor bori ginsburg scale sgd batch size train url address correspond martin institut formal appli linguist faculti mathemat physic charl univ czech republ