transform time seri survey academi group bellevu usa academi group hangzhou china key lab intellig shanghai tong univ abstract transform achiev superior perform mani task natur languag process comput vision also trigger great inter est time seri commun among multipl advantag transform abil captur long rang depend interact attract time seri model lead excit progress variou time seri cation paper systemat review transform scheme time seri model highlight strength well limit particular examin develop time seri transform two perspect perspect network structur summar adapt cation made transform order accommod time seri analysi perspect applic categor time seri tran former base common task includ forecast anomali detect cation perform robust analysi model size anal season trend decomposit analysi studi transform perform time seri discuss suggest futur direct provid use research guidanc correspond resourc continu updat found github introduct innov transform deep learn et al brought great interest recent due excel perform natur languag process kenton other comput vision cv et al speech process dong et al past year numer transform propo advanc state art per variou task quit literatur review differ aspect applic han et al cv applic han et al transform al shown great model abil long rang depend interact sequenti data thu appeal time seri model mani variant transform propo address special time seri model success appli variou time seri task forecast li et al et al anomali detect al et al cation et al yang et al season pe import featur time seri wen et al effect model long rang short rang tempor depend captur season simultan remain challeng al wen et al note exist sever survey relat deep learn time seri includ forecast et al torr et al cation mail et al anomali detect et al bl al data wen et al comprehen transform time seri transform time seri emerg subject deep learn systemat comprehen survey time seri transform would greatli time seri commun paper aim gap summar main develop time seri transform give brief introduct vanilla transform propo new taxonomi perspect network cation applic domain time seri tran former network cation discuss improv made low level modul high level architectur transform aim optim perform time seri model applic analyz summar transform popular time seri task includ forecast anomali detect cation time seri transform analyz insight strength limit provid practic guidelin effect use transform time seri model conduct exten empir studi examin multipl aspect time seri model includ robust analysi model size analysi season trend decomposit analysi conclud work discuss possibl futur direct time seri transform induct bia time seri transform tran former time seri may time seri transform architectur level variant transform time seri best knowledg work comprehen systemat review key develop transform model time seri data hope survey ignit research interest time seri transform preliminari transform vanilla transform vanilla transform et al follow competit neural sequenc model encod decod structur encod decod compo multi ident block encod block consist multi head self attent modul posit wise feed forward network decod block insert cross attent model multi head self attent modul posit wise feed forward network input encod posit encod unlik vanilla transform recur instead util posit encod ad input emb model sequenc inform summar posit absolut posit encod vanilla transform posit index encod vector given pe sin co hand craft frequenc dimen anoth way learn set posit emb posit kenton other et al rel posit encod follow intuit pairwi posit relationship input element posit element rel posit encod method propo exampl one method add rel posit emb key attent mechan shaw et al besid absolut rel posit method use hybrid posit combin togeth al gener encod ad token emb fed transform multi head attent queri key valu model scale dot product attent use transform given attent q k v v queri q key k valu v n denot length queri key valu dk dimen key queri valu transform use multi head attent time seri taxonomi transform time seri model perspect network cation applic domain set learn project instead sin attent function q k v head head h wo head feed forward residu network feed forward network fulli connect modul h output previou layer trainabl pa deeper modul residu connect modul follow layer normal modul insert around modul x h denot self attent modul denot layer normal oper taxonomi transform time seri summar exist time seri transform pro pose taxonomi perspect network cation applic domain illustr fig base taxonomi review exist time seri transform systemat perspect network cation summar chang made modul level architectur level transform order ac special challeng time seri model perspect applic classifi time seri tran former base applic task includ forecast anomali detect cation follow two section would delv exist time seri transform two perspect network cation time seri posit encod order time seri matter great encod posit input time seri tran former common design encod posit vector inject model input togeth input time seri obtain vector model time seri tran former divid three main categori vanilla posit encod work al simpli introduc vanilla posit encod section use et al ad input time seri emb fed transform al though approach extract posit inform time seri unabl fulli exploit featur time seri data posit encod vanilla encod hand craft less express sever studi found learn appropri emb time seri data much effect compar vanilla posit encod learn emb adapt c task et al introduc emb layer transform learn emb vector posit index jointli model paramet et al use network encod posit em better exploit sequenti order time seri timestamp encod model time seri real world scenario timestamp inform com access includ calendar timestamp sec minut hour week month year special time tamp holiday event timestamp quit inform real applic hardli leverag vanilla transform mitig issu inform et al propo encod timestamp addit posit encod use emb layer similar timestamp encod scheme use al et al attent modul central transform self attent modul view fulli connect layer weight dynam gener base pairwi similar put pattern result share maximum path length fulli connect layer much less paramet make suitabl model long term depend show previou section self attent mod vanilla transform time memori com input time seri length becom comput bottleneck deal long sequenc mani transform propo reduc quadrat complex ed two main categori explicitli introduc sparsiti bia attent mechan like al al explor low rank properti self attent matrix speed inform et al et al tabl show time memori com popular transform appli time seri mod detail model discuss section complex comparison popular time seri transform differ attent modul test time memori step transform et al n al inform et al al al n n chen et al et al n n n architectur base attent innov accommod individu modul transform model time seri number work et al al seek renov transform level recent work introduc hierarch transform take account multi resolut aspect time seri inform et al insert max pool layer stride attent block down sampl seri half slice et al design tree base attent node nest scale correspond orig time seri node coarser scale repr seri lower resolut develop scale inter scale attent order better captur depend across differ resolut besid abil integr inform differ multi resolut hierarch architectur also enjoy ts comput particularli long time seri applic time seri transform section review applic transform import time seri task includ forecast anomali detect cation transform forecast examin three common type forecast task time seri forecast spatial tempor forecast event forecast time seri forecast lot work done design new transform variant time seri forecast task latest year modul level architectur level variant two larg categori former consist major date work modul level variant modul level variant time seri forecast main architectur similar vanilla transform minor chang research introduc variou time seri induct bia design new modul follow summar work consist three differ type design new attent modul explor innov way normal time seri data util bia token input shown figur type variant modul level transform design new attent modul categori largest proport describ six typic work al inform et al figur categor modul level transform variant time seri forecast al al former chen et al et al exploit sparsiti induct bia low rank approxim remov noi achiev low order cal complex al propo con self attent employ causal convolut gener queri key self attent layer introduc spar bia mask self attent model reduc comput complex instead use explicit spar bia former et al select domin queri base queri key similar thu achiev similar improv comput complex also de sign gener style decod produc long term fore cast directli thu avoid accumul error us one forward step predict long term forecast al use gener adversari encod decod framework train spar transform model time seri forecast show adversari train improv time seri forecast directli shape output distribut network avoid error one step ahead infer al design hierarch pyramid attent mod binari tree follow path captur tempor depend differ rang linear time mem complex et al appli oper frequenc domain fourier tran form wavelet transform achiev linear complex randomli select subset frequenc note due success attract attent commun explor self attent mechan frequenc domain time seri model chen et al propo learn rotat attent base quaternion introduc period phase inform depict period pattern moreov decoupl use global memori achiev linear complex follow three work focu build explicit interpret abil model follow trend explain intellig et al design multi horizon forecast model static encod gate featur select tempor self attent decod encod select use variou perform forecast also preserv incorpor global depend event tang al combin transform state space model provid probabilist forecast design gener model infer base variat infer use tran former learn tempor pattern estim ssm appli ssm perform season trend decomposit maintain interpret abil second type variant modul level transform way normal time seri data best knowledg non stationari transform al work mainli focu modifi normal mechan shown figur explor over problem time seri forecast task rel simpl plugin seri stationari de stationari modul modifi boost perform variou block third type variant modul level transform util bia token input al adopt segment base repr devi simpl season trend decomposit ar auto correl mechan work attent modul auto correl block measur time delay similar input signal gate top k similar sub seri produc output reduc complex et al util channel independ channel contain singl time seri share emb within seri subseri level patch design mentat time seri subseri level patch serv input token transform et al alik design improv numer long time time seri forecast task lot cross former propo transform base model util cross dimen depend multivari time seri forecast input emb vector array novel dimen segment wise em bed preserv time dimen inform two stage attent layer use captur cross time cross dimen depend architectur level variant work start design new transform architectur beyond scope vanilla transform et al design triangular c patch attent triangular tree type structur later input size shrink exponenti set c paramet make multi layer maintain lightweight linear complex et al propo multi scale framework appli baselin transform base time seri forecast model et al al etc improv baselin perform iter fore time seri multipl scale share note et al question necess use transform long term time seri forecast show simpler model achiev better result compar transform base line empir studi howev notic re cent transform model al achiev better numer result compar long term time seri forecast moreov thorough studi et al show tran former model univ sequenc sequenc function question type method time seri forecast base sole experi result variant method especi transform model alreadi demonstr perform machin learn base task therefor conclud recent transform base model time seri fore cast necessari would whole commun forecast forecast tempor tempor depend taken account time seri transform accur forecast c transform al design encod decod structur use self attent modul captur tempor tempor depend graph neural network modul captur spatial depend spatial tempor transform al c ow forecast take step besid introduc tempor transform block captur tempor depend also design spatial transform block togeth graph network better captur spatial spatial depend graph transform al design attent base graph convolut mechan abl learn complic tempor spatial attent pattern improv pedestrian trajectori predict gao et al propo cuboid attent space time model decompo data cuboid appli cuboid level self attent parallel show achiev superior perform weather climat forecast recent et al devi dartboard spatial self attent modul causal tempor self attent modul captur spatial correl tempor depend respect fur enhanc transform latent variabl captur data uncertainti improv air qualiti forecast event forecast event sequenc data irregular asynchron time tamp natur observ mani real life applic contrast regular time seri data equal sampl interv event forecast predict aim predict time mark futur event given past event often model tempor point process al et al recent sever neural model incorpor tran former order improv perform event diction self attent process et al transform process et al adopt transform encod architectur histor event comput inten function event predict modifi posit en code translat time interv sinusoid function interv event util later name attent neural time et al propo extend scheme emb possibl event time well experi show better captur sophist event depend exist method transform anomali detect transform base architectur also ts time anomali detect task abil model depend bring high detect qualiti xu et al besid multipl studi includ et al ae wang et al et al research pro pose combin transform neural gener model v well good fellow et al better perform anomali elabor model follow part et al propo adversari train procedur amplifi reconstruct error sim transform base network tend miss small anomali gan style adversari train procedur design two transform encod two tran former decod gain stabil ablat studi show transform base encod decod replac score drop nearli indic effect transform time seri anomali detect ae wang et al et al combin v ae transform share differ purpo combin v ae transform allow reduc train cost nearli ae transform design extract integr time seri inform differ scale overcom shortcom tradit transform local inform extract sequenti analysi chen et al combin transform graph base learn architectur multivari time seri anomali detect note ae also multi variat time seri dimen close relationship among sequenc graph network model work well deal challeng ae es posit encod mod introduc featur learn modul instead contain graph convolut structur model propag process similar ae also inform yet replac vanilla multi head attent multi branch attent mechan combin global learn attent vanilla multi head attent neighborhood convolut al combin transform gaussian prior associ make anomali di share similar motiv achiev goal differ way insight harder anomali build strong associ whole seri easier adjac time point com pare normal prior associ seri associ model simultan besid reconstruct loss anomali model optim strategi constrain prior seri distinguish associ discrep transform cation transform prove effect variou time seri cation task due promin capabl long term depend al use two tower transform tower respect work time step wise attent channel wise attent merg featur two tower weight concaten also known use propo exten transform achiev state art result multivari time seri cation studi self attent base transform raw optic satellit time seri cation obtain best result compar recurr neural network recent et al design transform learn task awar data reconstruct augment cation perform util score import timestamp mask recon bring superior perform transform also investig ca task yuan lin studi transform raw optic satellit imag time seri cation use self supervi schema label data et al introduc framework model proport mask data model ne tune downstream task cation yang et al propo use larg scale speech process model downstream time seri cation problem gener competit result popular time seri cation experi evalu discuss conduct preliminari empir studi typic benchmark et al analyz transform work time seri data sinc classic statist model basic model perform inferior transform shown et al al focu popular time seri transform differ con experi robust analysi lot work describ care design attent modul lower quadrat calcul memori com though practic use short input achiev best result report experi make us question actual usag design perform robust experi prolong input sequenc length verifi predict power robust deal long term input sequenc tabl tabl compar predict result prolong input length variou transform base comparison robust experi fore cast step prolong input length model transform inform reform tabl comparison model size experi fore cast step differ number layer model transform inform reform deterior quickli phenomenon make lot care fulli design transform impract long term fore cast task sinc effect util long input inform work design need investig fulli util long sequenc input better perform model size analysi introduc eld time seri transform shown domin perform cv commun et al kenton han et al han et al one key advantag transform hold abl increa predict power increa model size usual model capac control layer number commonli set yet shown experi tabl compar predict result differ transform model variou number layer transform layer often achiev better result rai question design proper transform architectur deeper layer increa capac achiev better forecast perform season trend decomposit analysi recent studi research al et al lin et al et al begin realiz season trend decomposit cleveland et al wen et al crucial part time seri forecast experi shown tabl adopt simpl move averag season trend decomposit architectur propo al test variou attent modul seen simpl season trend decomposit model boost perform uniqu block perform boost decomposit seem consist phenomenon time seri forecast tran applic worth investig advanc care design time seri scheme futur research opportun highlight direct potenti promi futur research transform time comparison ablat experi season trend decomposit analysi mean origin version without decomposit mean decomposit experi perform prolong output length model inform reform transform promot induct bia time seri transform vanilla transform make assumpt data pattern characterist although gener univ network model long rang also come price lot data need train transform improv gener avoid data one key featur time seri data trend pattern wen et al cleveland et al recent studi shown incorpor seri period al process et al time seri tran former enhanc perform moreov interest studi adopt seemli opposit bia achiev good numer improv al remov cross channel depend util channel independ attent modul interest work improv mental perform util cross dimen depend two stage attent mechan clearli noi signal cross channel learn paradigm clever way util induct bia suppress noi extract signal still desir thu one futur consid effect way induc induct bia transform base understand time seri data characterist c task transform time seri multivari time seri becom increasingli common applic call addit techniqu handl high dimen especi captur underli relationship among introduc graph neural network natur way model spatial depend relationship among di recent sever studi demonstr combin could bring cant perform improv like c forecast al xu et al multi modal forecast al also better understand dynam latent causal import futur direct combin transform effect spatial tempor model time transform time seri larg scale transform model boost perform variou task kenton other brown et al cv chen et al howev limit work transform time seri exist stud mainli focu time seri cation et al yang et al therefor develop transform model differ task time seri remain examin futur transform architectur level variant develop transform model time seri main tain vanilla architectur cation mainli attent modul might borrow idea transform variant cv also architectur level model design differ purpo lightweight al et al cross block connect et al adapt com time et al et al recurr al therefor one futur direct consid architectur level design transform optim time seri data task transform time seri hyper paramet emb dimen number larg affect perform transform manual con hyper paramet time consum often result suboptim techniqu like neural architectur search et al wang et al popular techniqu discov effect deep neural autom transform design use cv found recent studi al chen et al industri scale time seri data high dimen long length automat discov memori tran former architectur practic import make import futur direct time seri transform conclu provid survey time seri transform organ review method new taxonomi consist network design applic summar method categori discuss strength limit experi evalu highlight futur research direct refer et al mia xu chen yuan wu train deeper neural machin translat model transpar attent et al valentin wang daniel et al deep learn time seri forecast tutori literatur survey comput survey al ane bl angel et al review detect time seri data comput survey brown et al tom brown benjamin mann nick ryder melani jare et al model few shot learner al ling mai c transform captur period time seri c forecast tran action chen et al chen wang chang xu deng xu et al imag process transform chen et al chen peng ling search transform vi recognit chen et al chen chen yuan learn graph structur transform multivari time seri anomali detect ieee internet thing journal chen et al chen wang peng wen sun learn rotat quaternion transform complic period time seri forecast et al park deep learn anomali detect time seri data review analysi guidelin ieee access et al roy shang k gupta hong task awar reconstruct time seri transform et al bin yang dong pan angular c attent long sequenc ate time seri forecast cleveland et al robert cleveland william cleveland jean et al season trend decomposit procedur base loess journal statist al yang yang jaim g v le et al transform xl attent model beyond context et al stephan kaiser univ tran former dong et al dong xu xu speech transform recurr sequenc sequenc model speech recognit et al luca beyer alexand dirk thoma et al imag worth word transform imag recognit scale et al thoma jan frank neural architectur search survey journal machin learn research gao et al gao wang berni wang mu li et al explor space time transform earth system forecast et al jona michael david deni n dauphin sequenc sequenc learn et al ian jean mirza xu david et al gener adversari net han et al xu han ding et al train model past present futur ai open han et al han wang chen chen tang et al survey vision transform ieee rob j automat time seri forecast forecast pack age r journal statist softwar ismail et al ismail jonathan weber pierr muller deep learn time seri cation review data mine knowledg discoveri al di rethink posit encod languag kenton other jacob ming wei chang kenton et al bert deep bidirect transform languag understand well p max well auto encod variat al li chen wang enhanc local break memori bottleneck transform time seri forecast al li li tong david grin gener relat intent network multi agent predict et al wang wen roger predict nationwid air qualiti china transform bryan stefan time seri forecast deep learn survey philosoph transact royal societi et al bryan nicola toma p ster tempor fusion transform interpret multi horizon time seri forecast intern journal forecast al yang lin state space decomposit neural network time forecast al chen wang wei song gate tran former network multivari time seri cation al hang cong liao li lin alex x low complex pyramid attent long rang time seri model forecast al yong wu wang ming long non stationari transform explor station time seri forecast et al luke delight deep light weight transform al yang jason transform emb irregularli space event particip al nam h nguyen time seri worth word long term forecast transform k marc marco k self attent raw optic satellit time seri cation j remot sen et al amin amir tristan iter multi scale tran former time seri forecast shaw et al peter shaw self attent rel posit repr et al ali caner tim stephan g neural tempor point process review al david le chen evolv transform tang tang david transform time seri analysi al don transform survey comput survey torr et al f torr se baa francisco mart alicia deep learn time seri forecast survey big data al nichola r jen deep transform network anomali de multivari time seri data et al par mar jone n gomez kaiser et al attent need wang et al wang xi yang et al merg oper one differenti architectur search wang et al wang pi et al variat transform base anomali detect approach multivari time seri measur page wen et al wen gao song sun xu et al robust season trend decomposit algorithm long time seri wen et al wen li sun fast robust season trend posit time seri complex pattern wen et al wen sun min et al time frequenc mine robust multipl detect wen et al wen sun fan yang song gao wang xu time seri data augment deep learn survey wen et al wen yang sun robust time seri analysi applic perspect al wu xi ding wei huang adversari spar tran former time seri forecast al wu lin lin song han lite transform long short rang attent al wu xu wang ming long decomposit transform auto correl long term seri forecast al raphael tang lee jimmi j lin dynam earli exit infer al xu gao lin qi spatial tempor transform network c ow forecast al xu wu wang ming long anomali transform time seri anomali associ discrep al xu li mod applic tempor point process yang et al huck yang pin chen v reprogram acoust model time seri cation al graph transform network trajectori predict yuan lin yuan yuan lei lin self supervi train transform satellit imag time seri ca ieee j star al singh j et al transform univ sequenc sequenc function et al ail chen lei xu transform effect time seri forecast et al georg patel transform base framework multivari time seri learn cross former transform util cross dimen depend multivari time seri forecast et al aldo omer self attent process et al et al anomali detect multivari time seri transform base variat et al peng li hui former beyond transform long sequenc time seri forecast et al wen wang sun frequenc en decompo transform long term seri forecast al li transform process 