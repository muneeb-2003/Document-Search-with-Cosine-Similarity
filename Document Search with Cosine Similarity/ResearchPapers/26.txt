ieee transact neural network learn system vol juli stabl featur select algorithm li member ieee jenni si fellow ieee huang chen abstract two factor charact good featur select algorithm accuraci stabil paper aim new approach stabl featur select innov paper center class stabl algorithm call featur weight learn stabil properti regular investig addit commonli adopt implement strategi ensembl propo stabil bound ensembl also present experi sourc real data challeng small sampl size problem demonstr ensembl stabl also compar accuraci popular weight method index term energi base learn ensembl featur select featur weight uniform weight stabil i featur select activ research area machin learn data decad import frequent use techniqu data remov irrelev redund inform data set also knowledg discoveri tool provid insight problem interpret relev featur discuss featur usual center two technic aspect search evalu criteria algorithm design differ strategi broadli fall three categori wrapper hybrid emb model hand categor base output characterist select algorithm divid either featur algorithm subset select algorithm paper focu featur weight comprehen survey exist featur select techniqu uni cation found addit cation accuraci anoth import measur stabil evalu qualiti featur manuscript receiv august revi april accept juli date public august date current work support part nation foundat china r grant grant grant grant grant part nation scienc foundat grant grant part govern scholarship part project li g s huang high technolog research key laboratori wireless sensor network colleg scienc nanj univ post telecommun nanj china e mail j si school electr comput energi engin arizona state univ temp az usa e mail si s chen colleg comput scienc technolog nanj univ aeronaut astronaut nanj china e mail digit object er algorithm stabil mean insensit result featur select algorithm variat train data set issu particularli import applic featur select use knowledg discoveri tool identifi characterist mark explain observ phenomena featur without stabil constraint usual result differ featur subset due variat train data even though featur subset term cation accuraci unstabl featur select result shake con domain expert experi valid select featur interpret import discoveri instanc analyz cancer leukemia avail data high dimen yet small sampl size among thousand genet express level critic subset discov link two leukemia therefor necessari e select predict gene common variat train sampl otherwi result lead less con dent diagnosi consid import stabil applic sever stabl select algorithm propo sampl weight featur group exampl comprehen survey earlier work found featur select algorithm make use empir criteria stabil measur fell short explicitli provid stabil analysi press need analyt stabl featur select algorithm beyond simpl empir approach thu evid paper guid energi base learn new algorithm framework e weight regular energi base learn propo stabil algorithm examin addit ensembl also introduc analyz stabil propo appli open sourc real data demonstr effect stabil accuraci high small sampl size applic problem paper organ follow framework ensembl introduc section iii analyz stabil featur weight addit stabil analysi ensembl present experi result data shown section iv paper conclud section v ii e base learn featur weight energi base learn provid uni ed framework mani probabilist approach person u perm requir ieee permiss see inform author licen use limit nation univ fast download februari utc ieee restrict appli al stabl featur select algorithm learn predict cation deci make sampl rank detect condit densiti estim paper consid energi base learn framework design featur weight algorithm focu develop ensembl also discuss addit featur weight algorithm well implement algorithm stabil analysi algorithm energi base learn consid infer model input variabl infer variabl good possibl model con relat measur energi function e x valu energi function view degr compat given con convent small energi valu correspond highli compat con larg energi valu correspond highli incompat con g appli infer model x model produc compat answer ye x energi base learn entail energi function produc best v e n x search best energi function famili energi function form e w x w w propo model paramet b regular energi base learn train energi base model given train x xi n train input correspond desir answer label limit sampl input repr d dimen vector xi rd best energi function famili f need assess qualiti energi function inform train set possibl prior knowledg task data qualiti measur loss function function function denot l w call object loss function accordingli learn problem becom minim object loss w usual object loss function base data set di de follow ld w w xi r w right hand side l w xi per sampl loss function term n w xi sampl averag loss function taken n respect per sampl loss function denot jd w simplic jd w w xi w regular term use emb prior knowledg energi function other paper classic examin paramet cost balanc factor base discuss energi base learn evid per sampl loss function design way assign low loss well behav energi function energi function give lowest correct answer higher energi includ incorrect answer conver energi function assign lowest energi correct answer would high loss gener margin loss function exampl meet condit thu use per sampl loss function l w x introduc gener margin loss function follow de need de discret variabl offend incorrect answer one lowest energi among answer incorrect w xi gener margin loss function per sampl loss describ follow l w xi e w xi e w xi e w xi consequ denot notat cation energi correct answer e w xi denot energi offend incorrect answer xi posit paramet call margin energi gap answer correct one discuss function q assum convex easili ed moreov consid energi e n e q q gradient q along respect gener hold true q q region energi space mean wherev smaller gradient along larger gradient along q push valu pull valu cau q loss surfac slant toward low valu high valu meet cation section ii energi valu compat answer small energi valu incompat answer larg remark mani possibl realiz q hing log squar exponenti loss exampl log loss nite margin squar squar loss margin select correspond per sampl loss function follow l w xi e w xi e w xi l w xi w xi max e w xi paramet win energi function de featur weight vector featur weight w minim object loss de section iv d log squar squar loss chosen two repr per sampl loss author licen use limit nation univ fast download februari utc ieee restrict appli ieee transact neural network learn system vol juli function construct object loss function deriv c featur weight algor howev theoret result obtain paper limit log squar squar loss featur weight stabil analysi c featur weight regular energi base learn short featur weight problem consid energi base learn problem paramet win energi function relev featur second gener margin loss function select per sampl loss function l w x ss h w n construct object loss function ld w correct answer offend incorrect answer sampl ed summar follow issu critic featur design use appropri criteria determin correct answer offend incorrect answer sampl properli design structur energi function e need per sampl loss function consequ properli issu learn lead appropri energi function per sampl loss function push correct answer pull maintain energi gap margin address issu develop appropri determin correct answer offend incorrect answer sampl resort nearest neighbor cation scheme note er nonlinear map input pattern class label simpl algorithm receiv consid attent aga recent sinc demonstr highli state art real world applic addit er view energi base learn energi function sampl distanc measur consid sampl x class denot nh xi determin easili long distanc measur de also nh xi consid nearest correct answer similarli differ label denot nm xi consid offend incorrect answer base de correct offend incorrect answer ed nh xi nm xi respect energi function e w xi w xi need gener margin loss function de weight manhattan distanc shown follow e w xi w nh xi xi xi e w xi w nm xi xi xi denot element wise ab solut valu oper compon argument vector dimen din case transpo paramet associ energi function discuss solut gener problem energi function shown becom featur weight vector w w w current problem set algorithm algorithm step input train data set xi n xi rd margin regular paramet step initi rd step r n given xi n h e nh xi xi base algorithm b calcul e w nh xi xi e w nm xi xi obtain gener margin loss q l w xi n c n l w xi r w w step output featur weight vector optim featur weight found mani differ optim approach exampl algorithm use illustr minim object loss function next discuss place posit summar propo algorithm energi function de respect employ gener margin loss function form per sampl loss function l w x discuss mani differ gener margin loss function mention remark hing log loss integr differ make differ function l w therefor famili featur weight algorithm could consequ deriv note local learn base e weight algorithm describ special case log loss adopt moreov log loss use enhanc diver among base featur selector ensembl featur select obtain anoth special case note howev time algorithm includ analyz energi base learn perspect propo framework respect stabil properti provid also point purpo paper use manhattan distanc determin de ne energi function nonetheless standard distanc measur euclidean distanc candid without creat problem obtain result paper summar resort cation featur weight problem describ regular energi base learn featur weight vector correspond paramet win object loss function de gener margin loss function q convex adopt per sampl loss function l w xi energi base learn object loss function classic consid paper author licen use limit nation univ fast download februari utc ieee restrict appli al stabl featur select algorithm algorithm ensembl step input train data set xi n xi rd margin regular paramet random sampl paramet step initi rd step r produc bootstrap subset rt size b perform rt obtain base weight result rt c rt step output ensembl featur weight result d ensembl ensembl learn effect approach produc robust accur learn solut machin learn demonstr mani cant instanc popular ensembl use bag approach consist averag sever estim built random origin data set similar ensembl model supervi learn two essenti step ensembl featur select step involv creat set differ base featur selector provid output second step aggreg result base featur selector case bootstrap base strategi use train base featur selector deriv bootstrap subset origin train set x n ensembl featur weight result achiev averag obtain solut base featur selector let n integ closest r l e rt rt rt index sequenc randomli draw n natur sequenc n without replac denot subset rt k k subset drawn independ rt denot outcom featur weight algorithm appli bootstrap train subset rt therefor obtain featur weight result w rm paper ensembl result obtain mm rt aggreg averag output base featur selector discuss ensembl provid algorithm iii analysi paper stabil consid follow sen variat output small respon small variat input data set may entail follow two scenario perturb instanc level cau exampl remov sampl ad sampl data set second perturb featur level cau exampl ad noi featur data set addit combin type perturb data set cau stabil concern stabil sever cation regress sampl rank method analyz thoroughli sen similar howev featur select algorithm examin empir paper aim pr theoret analysi stabil featur weight algorithm account small instanc level perturb need consid remov one sampl data set analyz stabil properti featur stabil consid ad sampl fol low directli result remov sampl account small featur level perturb need consid chang one sampl examin impact stabil algorithm proceed analyz scenario describ consid follow given train set size certain distribut p sampl drawn independ ident distribut e ed train data set remov train sampl xi origin train data set w h e r e n denot train set obtain chang one sampl xi de consid featur weight algorithm output featur weight vector denot data set respect algorithm weight stabl stabil bound size n w eh v e intuit smaller valu greater stabil consid stabil properti algorithm data sampl distort origin data sampl train data set chang w el e h e featur weight vector output algorithm data set di base uniform weight stabil de appli triangl inequ therefor accord stabil chang one sampl reduc analyz stabil sampl word uniform weight stabil formul remov one train data sampl impli stabil con condit one sampl deviat origin data sampl sen de use analyz algorithm perturb instanc featur level describ begin section author licen use limit nation univ fast download februari utc ieee restrict appli ieee transact neural network learn system vol juli follow section discuss stabil stabil ensembl respect stabil analysi section examin stabil properti w remark part stabil analysi given choic energi function use shorthand notat l place per sampl loss function l w xi consid tran format xi differ loss function remark express differ instanc log loss use xi xi nm xi furthermor train sampl bound normal well denot accord remark object function ld w w de follow ld w w j theorem consid given train set e sampl xi correspond transform remark n assum per sampl loss function l lipchitz constant e featur weight result minim convex object function ld w w respect uniformli weight stabl proof refer appendix remark theorem show uniform weight stabil furthermor stabil bound approach zero therefor tight bound remark consid stabil sen de case remov correspond stabil bound obtain similarli singl sampl remov bound singl sampl remov well b stabil analysi turn stabil analysi due natur featur select algorithm usual result spar featur vector output contain element zero xu et al prove sparsiti stabil odd cation regress problem show spar algorithm stabl de algorithm suscept small variat input also prove spar algorithm identifi redund featur mean two featur highli depend r remov one featur would affect class discrimin power algorithm therefor spar algorithm may optim solut thu may ill pose paper constraint stabil sen de preserv given train set distribut p assum exist true uniqu unknown featur weight vector w e optim estim w respect optim refer optim solut result minim object loss function ld w w respect ld w w w j w  carri discuss uniform weight stabil de ne exactli spar model de featur weight featur weight vector exactli zero featur weight model spar moreov analyz stabil also need addit condit sampl averag loss function jd w n w w n j w differ satisfi strong convex condit de de sampl averag loss function j w convex paramet jd w w jd w inner product similarli de ne strong convex w paramet remark refer remark elabor per sampl loss function l w among log squar squar squar exponenti loss differenti therefor correspond sampl averag loss function jd w w also differenti remark differenti per sampl loss function remark evid squar exponenti squar squar loss strongli convex introduc author licen use limit nation univ fast download februari utc ieee restrict appli al stabl featur select algorithm log loss also strongli convex correspond sampl averag loss function jd w w also strongli convex theorem consid given train set distribut e n input sampl xi rd n true uniqu unknown featur weight vector assum sampl averag loss function j w w differenti strong convex paramet n de let spar featur weight result minim convex object function ld w w respect paramet jd w w uniformli weight stabl proof refer appendix b remark output exactli spar sampl averag loss function strongli convex e featur weight stabil bound inver affect strong convex constant remark bound also scale regular paramet make sen sinc spar solut lead less featur weight stabil properti remark consid stabil de case remov let correspond sampl averag loss function strongli convex paramet st abil bound sampl obtain similarli singl sampl remov stabil bound c stabil ensembl base de uniform weight stabil ensembl propo section ii d de de given train data set x n n ensembl uniformli weight stabl stabil bound f e rm mm rt rm mm rt rt base featur weight result bootstrap subset rt whose size rt base featur weight result bootstrap subset rt xi n remov number bootstrap subset expect rt rt rt index sequenc randomli draw n natur sequenc n without replac theorem consid ensembl describ algorithm given data set sampl xi n bootstrap strategi adopt ling paramet creat subset rt size rt rt rt index drawn natur sequenc n without replac let uniform stabil bound base featur weight algorithm ensembl uniformli weight stabl bound proof refer appendix c remark theorem indic ensembl featur weight tighter stabil bound base featur weight consist observ ensembl strategi usual improv featur select stabil remark case remov correspond stabil bound obtain similarli singl remov sampl bound approxim w h e r e stabil bound base featur weight remov iv e section evalu stabil accuraci ensembl comparison popular weight algorithm four real life problem problem among challeng prob featur select particularli output stabil desir evalu illustr algorithm accuraci stabil problem analyz real world data includ tox leukemia prostat two data set download later avail goal analyz four problem identifi genet express link respect disea tox data set contain instanc gene consist myocard dilat cardiomyopathi infect male femal well uninfect male femal often cau viral occur frequent men infect increa risk die heart failur data set contain smoker either without lung cancer total number gene test primari disord bone marrow malign neoplasm stem cell leukemia data set analyz includ sampl acut patient either acut leukemia acut leukemia total number gene test prostat data set sampl prostat cancer patient gene studi among sampl tumor normal describ four data set tox leukemia prostat share common trait small sampl extrem high dimen latent variabl typic problem author licen use limit nation univ fast download februari utc ieee restrict appli ieee transact neural network learn system vol juli algorithm comparison problem gener accept featur select algorithm use obtain stabl featur output instead ensembl algorithm expect improv stabil proper tie featur select therefor focu provid algorithm conduct comparison paper howev still provid cation accuraci result use origin present algorithm comparison purpo consid three c algorithm describ algorithm log squar log consist log loss per sampl loss function use log algorithm per sampl loss function log use squar base squar squar loss function sampl xi h em r g n set manhattan distanc nm xi nh xi ensembl version three accord algor name en log en log en squar respect sinc focu paper featur weight therefor chose popular featur weight algorithm comparison algorithm includ relief fisher score en relief en fisher varianc reduct en relief en fisher ensembl version relief fisher score implement en relief en fisher similar describ algorithm relief fisher score place respect relief algorithm consid one featur select algorithm due simplic effect key idea relief iter estim featur weight accord abil neighbor sampl iter sampl x randomli select two found one class weight featur updat base distanc x two featur exten relief consid sever deal multipl class problem would like highlight relationship relief first algorithm view hypothesi margin base approach howev differ two algorithm evid systemat framework stabl featur select base energi base learn regular mani c algorithm view realiz consid relief relief directli calcul featur weight base margin distanc obtain featur weight base margin loss loss function select pool candid function stabil analysi provid time fisher score one wide use featur select method key idea fisher score featur data space span weight featur distanc data point differ class larg possibl distanc data point class small possibl fisher score prefer featur similar valu sampl class differ valu sampl differ class featur weight comput deviat featur mean valu class stabl featur weight algorithm use sampl weight strategi improv stabil featur sampl weight strategi introduc effect approach improv stabil featur select method assign differ weight sampl perform featur select aim paper combin sampl weight strategi obtain stabl featur weight algorithm present featur weight energi base model without regular word object function similar without r w squar squar loss mention remark use per sampl loss function l w x sever rang consid b stabil measur sinc almost featur select applic mate output featur selector subset featur consid promin therefor one usual make use correspond featur rank place featur weight perform evalu consid evalu stabil paper comput featur weight output weight turn featur rank therefor stabil measur use paper base featur rank statist cant sever differ set featur rank result obtain empir comput stabil measur ore use bootstrap base strategi without replac consid data set subset r l c z e drawn randomli bootstrap sampl without replac note name sampl subset sampl subset distinguish bootstrap subset weight procedur subsequ featur weight algorithm perform subset empha earlier featur weight result featur rank result calcul stabil measur algorithm result rank result r subset featur weight algorithm transform featur weight result sampl subset featur rank result rank valu featur determin follow best featur largest weight assign rank worst rank d author licen use limit nation univ fast download februari utc ieee restrict appli al stabl featur select algorithm ensembl featur weight featur rank obtain describ similar ensembl procedur describ algorithm sampl subset size still randomli sampl use bootstrap strategi without replac produc subset size n featur weight algorithm relief fisher score propo perform bootstrap subset obtain featur weight vector w obtain ensembl featur rank result ensembl algorithm sampl subset r l featur weight vector correspondingli transform featur rank vector base assign rule featur rank sampl subset c obtain averag respect bootstrap subset base featur rank consid featur rank vector set l l rd l c featur rank result sampl subset select stabil measur compar featur output subset similar output higher stabil measur over stabil de averag similar base pairwi similar differ featur rank result r c c c sim repr similar measur featur rank result featur rank spearman rank correl use calcul similar sim l c experi perform stabil base stabil measur procedur describ section iv b given data set sampl subset contain data randomli drawn without replac use bootstrap base strategi percentag chosen assess stabil respect rel small chang data set exampl leukemia randomli drawn use bootstrap base strategi without replac creat subset thu sampl subset contain patient sampl gene sampl subset ensembl featur weight algorithm en log en log en squar en relief en fisher appli describ section iv b obtain featur rank result simultan featur weight algorithm anoth method improv stabil featur select also appli sampl subset fig experi evalu stabil number base featur selector seven candid stabl featur sampl subset obtain featur rank result featur weight m featur weight algorithm similar featur rank result pair calcul use spearman rank c stabil featur weight algorithm thu averag similar pairwi similar calcul moreov examin effect regular paramet stabil exampl one origin algorithm deriv log one ensembl algorithm deriv ensembl en log chosen d experi result stabil examin effect number bootstrap subset use ensembl method name stabil measur see algorithm display trend stabil satur around sinc ensembl method stabil remain constant stabil result ensembl featur weight algorithm therefor shown fig fig propo ensembl en log en squar alway highest among algorithm algorithm en log lowest stabil consist observ sparsiti stabil odd hand examin effect regular paramet stabil log en log result leukemia prostat data set includ similar result obtain two data set includ due space experi result shown fig observ along increa stabil log en log improv consist theoret analysi e experi perform accuraci good featur selector b e stabl accur stabl featur select import consid author licen use limit nation univ fast download februari utc ieee restrict appli ieee transact neural network learn system vol juli fig experi evalu stabil function h e regular paramet log en log leukemia prostat mani applic cation accuraci use select featur accuraci evalu cation model base select featur experi conduct paper er er linear support vector machin polynomi kernel use cation model sinc gener easi appli good er use cation base featur weight introduc section iv cation accuraci assess use cross valid fold weight algorithm appli train data obtain featur rank result ensembl featur weight algorithm en log en log en squar en relief en fisher base result sec iv d fold bootstrap subset train data randomli drawn creat ensembl feat weight algorithm featur rank descend order differ number import featur select top rank one one creat note often observ data small amount featur gene relev thu number select import featur rank result less experi test result base obtain fold cation accuraci result obtain averag fold f experi result accuraci accuraci result provid case use select featur fig show summari raci valu regular e select algorithm relief fisher score log log squar use linear poli kernel er fig show summari valu algorithm design improv stabil en relief en fisher en log en log en squar use linear polynomi kernel er result summar fig show one algorithm consist better four test data set howev ensembl compar algorithm time fig experi result accuraci origin featur select method use differ b c linear polynomi g evalu stabil accuraci measur stabil cation accuraci featur select algorithm take refer robust perform author licen use limit nation univ fast download februari utc ieee restrict appli al stabl featur select algorithm fig experi result accuraci method design stabl featur select use differ b c linear polynomi measur featur stabil si cation accuraci paper de ne stabil accuraci trade off sat sat accuraci stabil stabil fig experi result stabil accuraci method design stabl featur elect use differ b c linear polynomi accuraci evalu use cation outcom base select featur stabil valu ensembl featur weight correspond accuraci number select featur author licen use limit nation univ fast download februari utc ieee restrict appli ieee transact neural network learn system vol juli use calcul experi result differ shown fig ensembl shown provid better stabil accuraci compar method v c paper propo new framework includ mani use stabl featur weight algorithm realiz also provid time theoret result uniform weight ensembl introduc mean improv stabil stabil also provid evalu perform make use three c realiz log log squar respect sever popular featur select algorithm includ comparison benchmark perform base problem experi result show ensembl use form algorithm stabil provid compar cation accuraci proof theorem proof let w h e r e featur weight result minim convex object function ld w w respect ld retain mini mum valu respect accordingli lead follow ld equat use replac correspond term n n j use shorthand notat n j ea discuss use togeth n n w d w d n n sinc per sampl loss function convex inequ l w d l l l similarli also obtain l l l substitut two ident lead n l l note inequ l l per sampl loss function l lipchitz therefor n n set left hand side amount thu n base inequ combin use w e obtain uniform stabil bound regular n author licen use limit nation univ fast download februari utc ieee restrict appli al stabl featur select algorithm appendix b proof theorem proof optim estim w respect optim refer optim solut result minim object loss function ld w w respect analyz two term start properti consid exact sparsiti de de featur weight result d dimen vector w w w weight exactli zero suppo number featur nonzero weight e index set whose correspond index featur exampl indic weight w zero let r real space de ne subspac r l l orthogon complement subspac r l l p remark subspac model subspac captur constraint ed ld w w orthogon complement subspac consid perturb subspac deviat away model subspac m readi de ne properti respect model subspac complement subspac de given subspac pair de respect abl respect next analyz bound simplifi notat drop subscript use proof let f w w w w sinc minim ld w satisfi f w w replac right hand side f w chang f w w w w note function f w consist two differ one sampl averag loss function jd w w w differ base subspac de evid w w h e r e w project w onto subspac orthogon complement subspac respect project oper de w w similarli obtain w h e r e project subspac respect de w n given analog manner w m triangl inequ w properti de obtain therefor exactli spar model de de ne model subspac w w guarant obtain turn differ b sampl averag loss function jd w w w w sd e n e analyz differ sampl averag loss function f w let sampl averag loss function jd w differenti satisfi strong convex de base strong convex jd w de function f w express f w w w w jd w w inequ applic jd w w jd w without loss gener assum jd w conclud jd w w thu f w author licen use limit nation univ fast download februari utc ieee restrict appli ieee transact neural network learn system vol juli triangl inequ henc f w note sinc project de similarli term sinc w end obtain f w discuss f w obtain chang notat h e n d analyz bound similarli let f f w w similar analysi f w obtain w base without loss gener assum jd w w obtain stabil bound featur weight appendix c proof theorem proof uniform stabil ensembl left side bound take norm insid expect inequ accord let convex function random variabl f e x f x case convex obtain e rm mm rt mm rt rm mm rt mm rt sinc suppo distribut r model bootstrap triangl inequ mm rt rt r r er r therefor accord ensembl stabil bound may consid similarli de remov singl sampl data set n c e sampl subset n need consid two possibl whether introduc indic function note r w h c h mean sampl bootstrap subset r r r term er r r follow r r r r r er r r r r size bootstrap subset r er r sampl done without replac r uniform stabil base featur weight result refer z featur n mine ultrahigh dimen data dissert dept deci syst eng arizona state u phoenix az usa h l integr featur select algorithm cation cluster ieee tran data eng vol pp apr i introduct variabl featur select j c h e r n e vol pp jan t van de peer featur select use ensembl featur select techniqu proc eur conf mach learn discoveri databa pp s l c ding group stabl featur select proc int conf discoveri data mine pp t t van de peer p cation f cancer diagnosi ensembl featur select method vol pp li s gao s chen featur weight base local learn diver proc conf pp p nguyen mine robust featur select proc int conf discoveri data mine pp han l varianc reduct framework stabl featur select proc int conf data mine dec pp l han m e abl gene select data via sampl weight tran biol vol pp l c ding s featur select via den featur group proc int conf discoveri data mine pp author licen use limit nation univ fast download februari utc ieee restrict appli al stabl featur select algorithm z w featur select discoveri biol chem vol pp s chopra r m f j huang tutori energi base model cambridg usa mit press s m data r s news person scalabl e collabor proc int conf world wide web pp s j t p discrimin method cation tum use gene express data j amer statist assoc vol pp sun s s son featur select high dimen data analysi ieee tran pattern anal mach vol pp sep p use thing know machin learn vol pp b wang h d ensembl optim input prune neural network use trust tech ieee tran neural vol pp jan s tang t wang t s ensembl learn concept detect ieee tran multimedia vol pp feb s gutta j r j huang p jonathon h expert cation r ethnic origin pose human face ieee tran neural vol pp jul l i j j rodriguez c o d e j linden s j johnston subspac cation ieee tran med vol pp feb l predictor mach learn vol pp aug o gener j mach learn re vol pp jan t m random learn algorithm j c h e r n e vol pp jan s p bound rank algorithm via algorithm stabil j c h e r n e vol pp feb h xu c s algorithm stabl free lunch theorem ieee tran pattern anal mach vol pp jan introductori lectur convex optim basic cour boston usa m tan i w l wang spar logist regress high dimen featur select ieee tran neural learn syst vol pp oct t r et al cation cancer class discoveri class predict gene express monitor scienc vol pp d singh et al express correl clinic prostat cancer behavior cancer cell vol pp k l practic approach featur select proc int workshop mach learn pp i attribut analysi exten relief proc eur conf mach learn pp m i empir analysi mach learn vol pp r o p e hart d g stork pattern cation ed new york ny usa wiley t g learn research four current direct ai mag vol pp k crammer r n analysi algorithm advanc neural inform process system cambridg usa mit press pp li b l lu select base loss margin nearest neighbor cation pattern vol pp j m hilario featur select algorithm studi high dimen space inf syst vol pp c c chang c j lin librari support vector machin onlin avail s van de gener linear model lasso ann statist vol pp s n p m j wainwright b uni ed framework high dimen analysi m estim decompo statist sci vol pp s n p m j wainwright b uni ed framework high dimen analysi m estim decompo dept univ california berkeley berkeley ca usa tech rep s n p m j wainwright b uni ed framework high dimen analysi m estim decompo advanc neural inform process system red hook ny usa associ inc pp li receiv degr comput scienc chongq univ chongq china post doctor fellow depart comput scienc engin shang tong univ shanghai china professor colleg comput scienc nanj univ post nanj china publish refer research paper current includ machin learn data mine parallel comput dr li co public chair intern confer neural inform process research current nation natur scienc china natur scienc foundat jenni si receiv degr univ beij china degr univ dame dame usa faculti depart electr engin arizona state univ phoenix az usa sinc also made new effort build capabl studi fundament neurosci question regard frontal cortex lab well import techniqu multichannel singl unit extracellular record use behav rat model chronic physiolog experi current research interest includ dynam optim use learn l network approxim approach name approxim dynam program dr si associ editor ieee semi conductor manufactur ieee automat control ieee neural network serv sever profess execut board confer committ vice presid educ ieee comput intellig societi advisor nsf social behavior cal directori serv sever propo review panel consult f intel arizona public servic distinguish lect ieee comput intel societi action editor neural network recipi nation scienc hou presidenti faculti fellow award motorola engin excel award receiv degr nanj univ post telecommun nanj china current pursu degr comput scienc current research interest includ machin learn huang receiv degr colleg univ mine technolog xuzhou china current pursu degr comput scienc nanj univ post nanj china current research interest includ machin learn author licen use limit nation univ fast download februari utc ieee restrict appli ieee transact neural network learn system vol juli chen receiv degr univ hangzhou china degr comput shanghai tong univ shanghai china degr inform system nanj aeronaut astronaut nanj china full time professor depart comput scienc engin sinc author co author c peer review paper h current research interest includ pattern recognit machin learn neural comput author licen use limit nation univ fast download februari utc ieee restrict appli 